{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0a33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASETS:  https://www.kaggle.com/competitions/daasbstp2023/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0847710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by importing all packages needed.\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import time\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.stats import uniform, poisson\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, \\\n",
    "                            accuracy_score, \\\n",
    "                            precision_score, \\\n",
    "                            recall_score, \\\n",
    "                            f1_score, \\\n",
    "                            fbeta_score, \\\n",
    "                            mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            roc_auc_score, \\\n",
    "                            roc_curve, \\\n",
    "                            classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4000a",
   "metadata": {},
   "source": [
    "# **Sample and Assess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b5b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets.\n",
    "df_en1 = pd.read_csv('daasbstp2023/energia_202109-202112.csv', na_filter=False, encoding = \"latin\")\n",
    "df_en2 = pd.read_csv('daasbstp2023/energia_202201-202212.csv', na_filter=False, encoding = \"latin\")\n",
    "df_me1 = pd.read_csv('daasbstp2023/meteo_202109-202112.csv', na_filter=False, encoding = \"latin\")\n",
    "df_me2 = pd.read_csv('daasbstp2023/meteo_202201-202212.csv', na_filter=False, encoding = \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6ff5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the datasets by type.\n",
    "df_en = df_en1._append(df_en2,ignore_index=True)\n",
    "df_me = df_me1._append(df_me2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfea1f",
   "metadata": {},
   "source": [
    "## Energy Consumption Dataset\n",
    "\n",
    "This dataset provides information on energy consumption recorded at different timestamps. Each record includes the following columns:\n",
    "\n",
    "- **Data (Date):** The timestamp associated with the record, indicating the date.\n",
    "- **Hora (Hour):** The hour associated with the record.\n",
    "- **Normal (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), from the electrical grid during a normal period in daily bi-hourly cycles (non-off-peak hours).\n",
    "- **Horário Económico (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), from the electrical grid during an economic period in daily bi-hourly cycles (off-peak hours).\n",
    "- **Autoconsumo (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), generated from solar panels (self-consumption).\n",
    "- **Injeção na rede (kWh):** A qualitative feature indicating the level of energy injection into the grid on a scale of None, Low, Medium, High, and Very High.\n",
    "\n",
    "This dataset is utilized in a competition and includes various features, with particular emphasis on the \"Injeção na rede (kWh)\" feature, which provides information about the qualitative scale of energy injection into the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb209be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11016 entries, 0 to 11015\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Data                     11016 non-null  object \n",
      " 1   Hora                     11016 non-null  int64  \n",
      " 2   Normal (kWh)             11016 non-null  float64\n",
      " 3   Horário Económico (kWh)  11016 non-null  float64\n",
      " 4   Autoconsumo (kWh)        11016 non-null  float64\n",
      " 5   Injeção na rede (kWh)    11016 non-null  object \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 516.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_en.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79924f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Normal (kWh)</th>\n",
       "      <th>Horário Económico (kWh)</th>\n",
       "      <th>Autoconsumo (kWh)</th>\n",
       "      <th>Injeção na rede (kWh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274</td>\n",
       "      <td>Very High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data Hora Normal (kWh) Horário Económico (kWh) Autoconsumo (kWh)  \\\n",
       "901  2021-11-05   13          0.0                     0.0             0.274   \n",
       "\n",
       "    Injeção na rede (kWh)  \n",
       "901             Very High  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2aa5b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data                       False\n",
       "Hora                       False\n",
       "Normal (kWh)               False\n",
       "Horário Económico (kWh)    False\n",
       "Autoconsumo (kWh)          False\n",
       "Injeção na rede (kWh)      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80133a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data, Number of Unique Values: 459\n",
      "Hora, Number of Unique Values: 24\n",
      "Normal (kWh), Number of Unique Values: 1282\n",
      "Horário Económico (kWh), Number of Unique Values: 851\n",
      "Autoconsumo (kWh), Number of Unique Values: 752\n",
      "Injeção na rede (kWh), Number of Unique Values: 5\n"
     ]
    }
   ],
   "source": [
    "# Unique values for each column in the Energy dataset.\n",
    "for column in df_en.columns:\n",
    "    unique_values = df_en[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105695bf",
   "metadata": {},
   "source": [
    "## Weather Information Dataset\n",
    "\n",
    "This dataset provides comprehensive information related to weather conditions at different timestamps. The columns in the dataset include:\n",
    "\n",
    "- **dt (Timestamp):** The timestamp associated with the record.\n",
    "- **dt_iso (ISO Date):** The date associated with the record, accurate to the second.\n",
    "- **city_name:** The location for which weather data is recorded.\n",
    "- **temp (Temperature):** The temperature in degrees Celsius.\n",
    "- **feels_like (Feels Like):** The perceived temperature in degrees Celsius.\n",
    "- **temp_min (Min Temperature):** The minimum felt temperature in degrees Celsius.\n",
    "- **temp_max (Max Temperature):** The maximum felt temperature in degrees Celsius.\n",
    "- **pressure:** Atmospheric pressure in atmospheres.\n",
    "- **sea_level:** Atmospheric pressure at sea level in atmospheres.\n",
    "- **grnd_level:** Atmospheric pressure at local altitude in atmospheres.\n",
    "- **humidity:** Humidity level as a percentage.\n",
    "- **wind_speed:** Wind speed in meters per second.\n",
    "- **rain_1h (Rainfall):** Average precipitation value in the last hour.\n",
    "- **clouds_all (Cloudiness):** Cloud coverage percentage.\n",
    "- **weather_description:** Qualitative assessment of weather conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c29965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11688 entries, 0 to 11687\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   dt                   11688 non-null  int64  \n",
      " 1   dt_iso               11688 non-null  object \n",
      " 2   city_name            11688 non-null  object \n",
      " 3   temp                 11688 non-null  float64\n",
      " 4   feels_like           11688 non-null  float64\n",
      " 5   temp_min             11688 non-null  float64\n",
      " 6   temp_max             11688 non-null  float64\n",
      " 7   pressure             11688 non-null  int64  \n",
      " 8   sea_level            11688 non-null  object \n",
      " 9   grnd_level           11688 non-null  object \n",
      " 10  humidity             11688 non-null  int64  \n",
      " 11  wind_speed           11688 non-null  float64\n",
      " 12  rain_1h              11688 non-null  object \n",
      " 13  clouds_all           11688 non-null  int64  \n",
      " 14  weather_description  11688 non-null  object \n",
      "dtypes: float64(5), int64(4), object(6)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_me.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a634c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>dt_iso</th>\n",
       "      <th>city_name</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_level</th>\n",
       "      <th>grnd_level</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>1633338000</td>\n",
       "      <td>2021-10-04 09:00:00 +0000 UTC</td>\n",
       "      <td>local</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.84</td>\n",
       "      <td>13.34</td>\n",
       "      <td>14.54</td>\n",
       "      <td>1023</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>2.07</td>\n",
       "      <td></td>\n",
       "      <td>69</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dt                         dt_iso city_name   temp feels_like  \\\n",
       "801  1633338000  2021-10-04 09:00:00 +0000 UTC     local  14.03      13.84   \n",
       "\n",
       "    temp_min temp_max pressure sea_level grnd_level humidity wind_speed  \\\n",
       "801    13.34    14.54     1023                            90       2.07   \n",
       "\n",
       "    rain_1h clouds_all weather_description  \n",
       "801                 69       broken clouds  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_me.iloc[801].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb4fdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                     False\n",
       "dt_iso                 False\n",
       "city_name              False\n",
       "temp                   False\n",
       "feels_like             False\n",
       "temp_min               False\n",
       "temp_max               False\n",
       "pressure               False\n",
       "sea_level              False\n",
       "grnd_level             False\n",
       "humidity               False\n",
       "wind_speed             False\n",
       "rain_1h                False\n",
       "clouds_all             False\n",
       "weather_description    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_me.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5bb4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt, Number of Unique Values: 11688\n",
      "dt_iso, Number of Unique Values: 11688\n",
      "city_name, Number of Unique Values: 1\n",
      "temp, Number of Unique Values: 2423\n",
      "feels_like, Number of Unique Values: 2702\n",
      "temp_min, Number of Unique Values: 488\n",
      "temp_max, Number of Unique Values: 550\n",
      "pressure, Number of Unique Values: 41\n",
      "sea_level, Number of Unique Values: 1\n",
      "grnd_level, Number of Unique Values: 1\n",
      "humidity, Number of Unique Values: 82\n",
      "wind_speed, Number of Unique Values: 771\n",
      "rain_1h, Number of Unique Values: 372\n",
      "clouds_all, Number of Unique Values: 101\n",
      "weather_description, Number of Unique Values: 8\n"
     ]
    }
   ],
   "source": [
    "# Unique values for each column in the Energy dataset.\n",
    "for column in df_me.columns:\n",
    "    unique_values = df_me[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ae71f",
   "metadata": {},
   "source": [
    "# **Data Processing**\n",
    "\n",
    "After Loading the data, we must now process it in order to be able to use it in our model. For this we will follow the following steps:\n",
    "1. Remove unnecessary columns\n",
    "2. Handle the Date column on both datasets and unify the format\n",
    "3. Join the datasets\n",
    "4. Handle the missing values or rows (dates that are not present in both datasets) if any\n",
    "5. Handle the categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35b1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop city_name, sea_level and grnd_level as they only have one unique value\n",
    "df_me = df_me.drop(['city_name', 'sea_level', 'grnd_level'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5dbb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal (kWh)</th>\n",
       "      <th>Horário Económico (kWh)</th>\n",
       "      <th>Autoconsumo (kWh)</th>\n",
       "      <th>Injeção na rede (kWh)</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274</td>\n",
       "      <td>Very High</td>\n",
       "      <td>2021-11-05 13:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Normal (kWh) Horário Económico (kWh) Autoconsumo (kWh)  \\\n",
       "901          0.0                     0.0             0.274   \n",
       "\n",
       "    Injeção na rede (kWh)             datetime  \n",
       "901             Very High  2021-11-05 13:00:00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert columns to unified format\n",
    "df_en['datetime'] = pd.to_datetime(df_en['Data'] + ' ' + df_en['Hora'].astype(str) + ':00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Drop the original 'Data' and 'Hora' columns if needed\n",
    "df_en = df_en.drop(['Data', 'Hora'], axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a60faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11016 entries, 0 to 11015\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Normal (kWh)             11016 non-null  float64       \n",
      " 1   Horário Económico (kWh)  11016 non-null  float64       \n",
      " 2   Autoconsumo (kWh)        11016 non-null  float64       \n",
      " 3   Injeção na rede (kWh)    11016 non-null  object        \n",
      " 4   datetime                 11016 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), object(1)\n",
      "memory usage: 430.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_en.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "669f1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>1633338000</td>\n",
       "      <td>2021-10-04 09:00:00</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.84</td>\n",
       "      <td>13.34</td>\n",
       "      <td>14.54</td>\n",
       "      <td>1023</td>\n",
       "      <td>90</td>\n",
       "      <td>2.07</td>\n",
       "      <td></td>\n",
       "      <td>69</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dt             datetime   temp feels_like temp_min temp_max  \\\n",
       "801  1633338000  2021-10-04 09:00:00  14.03      13.84    13.34    14.54   \n",
       "\n",
       "    pressure humidity wind_speed rain_1h clouds_all weather_description  \n",
       "801     1023       90       2.07                 69       broken clouds  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fix datetime in df_me (loses the UTC tag)\n",
    "df_me['dt_iso'] = pd.to_datetime(df_me['dt_iso'], format='%Y-%m-%d %H:%M:%S %z UTC')\n",
    "df_me['dt_iso'] = df_me['dt_iso'].dt.tz_localize(None)\n",
    "df_me = df_me.rename(columns={\"dt_iso\": \"datetime\"})\n",
    "df_me.iloc[801].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2b058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop the 'dt' column as it is redundant\n",
    "df_me = df_me.drop(['dt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21ffc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the dataframes by datetime so we can detect any time skips\n",
    "df_en = df_en.sort_values(by=['datetime'])\n",
    "df_me = df_me.sort_values(by=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b76b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irregular time intervals in df_en:\n",
      "0   NaT\n",
      "Name: datetime, dtype: timedelta64[ns]\n",
      "\n",
      "\n",
      "Irregular time intervals in df_me:\n",
      "0   NaT\n",
      "Name: datetime, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "time_diff_en = df_en['datetime'].diff()\n",
    "time_diff_me = df_me['datetime'].diff()\n",
    "\n",
    "# Print the irregular time intervals\n",
    "irregularities_en = time_diff_en[time_diff_en != '0 days 01:00:00']\n",
    "irregularities_me = time_diff_me[time_diff_me != '0 days 01:00:00']\n",
    "print(\"Irregular time intervals in df_en:\")\n",
    "print(irregularities_en)\n",
    "print(\"\\n\")\n",
    "print(\"Irregular time intervals in df_me:\")\n",
    "print(irregularities_me)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be20a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_merged_df = pd.merge(df_en, df_me, on='datetime', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8aa1ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal (kWh)               672\n",
       "Horário Económico (kWh)    672\n",
       "Autoconsumo (kWh)          672\n",
       "Injeção na rede (kWh)      672\n",
       "datetime                     0\n",
       "temp                         0\n",
       "feels_like                   0\n",
       "temp_min                     0\n",
       "temp_max                     0\n",
       "pressure                     0\n",
       "humidity                     0\n",
       "wind_speed                   0\n",
       "rain_1h                      0\n",
       "clouds_all                   0\n",
       "weather_description          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_join_merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602b4f6",
   "metadata": {},
   "source": [
    "Since the datetime data exhibited no irregularities, the additional entries present in the Weather dataset but not in the Energy dataset can be attributed to the Weather dataset containing data from days before or after the Energy dataset's first or last entry, respectively. A manual analysis of the dataset reveals that the Weather dataset includes entries starting from 2021-09-01, while the Energy dataset commences from 2021-09-29. Consequently, it is necessary to exclude entries from the Weather dataset that precede 2021-09-29, as they will not contribute to the modeling process. In order to achieve this we will do an Inner Join between the two datasets on the datetime column, and the resulting dataset will be the one we use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point, if I do an inner join I am losing 5.7% of all datarows\n",
    "merged_df = pd.merge(df_en, df_me, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sava the clean dataset\n",
    "merged_df.to_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'Injeção na rede (kWh)': 'Injection'}, inplace=True)\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805aa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec017bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'rain_1h' column to float and replace empty strings with 0\n",
    "merged_df['rain_1h'] = pd.to_numeric(merged_df['rain_1h'].replace('', '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ce984",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_description_counts = merged_df['weather_description'].value_counts()\n",
    "print(weather_description_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Encoding for 'weather_description'\n",
    "merged_df = pd.get_dummies(merged_df, columns=['weather_description'], prefix='weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(merged_df[['Normal (kWh)', 'Horário Económico (kWh)', 'Autoconsumo (kWh)', 'temp', 'rain_1h', 'humidity']])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343017dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = merged_df[['Normal (kWh)', 'Horário Económico (kWh)', 'Autoconsumo (kWh)', 'temp', 'rain_1h', 'humidity']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d832043",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Injection', y='temp', data=merged_df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd647e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df['datetime'], merged_df['Normal (kWh)'], label='Normal (kWh)')\n",
    "plt.plot(merged_df['datetime'], merged_df['temp'], label='Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Injection', y='humidity', kind='bar', data=merged_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11830f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='Injection', y='rain_1h', data=merged_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to visualize\n",
    "columns_to_visualize = ['Normal (kWh)', 'Horário Económico (kWh)', 'Autoconsumo (kWh)', 'temp', 'feels_like',\n",
    "                         'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'clouds_all']\n",
    "\n",
    "# Set up subplots with multiple columns\n",
    "num_columns = 3  # You can adjust the number of columns as per your preference\n",
    "num_rows = -(-len(columns_to_visualize) // num_columns)  # Ceiling division to determine the number of rows\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, 4 * num_rows))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# Flatten the axes array to simplify indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create individual plots for each column\n",
    "for i, column in enumerate(columns_to_visualize):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x='Injection', y=column, data=merged_df, ax=ax, hue='Injection')\n",
    "    ax.set_title(f'Injection vs {column}')\n",
    "    ax.set_xlabel('Injection')\n",
    "    ax.set_ylabel(column)\n",
    "\n",
    "# Remove empty subplots if needed\n",
    "for i in range(len(columns_to_visualize), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950e567",
   "metadata": {},
   "source": [
    "The problem is a classification problem with ordinal labels. We have the following possible values for the target variable: None, Low, Medium, High, Very High. Here are some suitable models for ordinal classification:\n",
    "\n",
    "1. **Ordinal Logistic Regression:**\n",
    "   - **Type:** Supervised learning, ordinal classification.\n",
    "   - **Strengths:** Designed specifically for ordinal outcomes, interpretable, and extends logistic regression to handle ordered categories.\n",
    "   - **Considerations:** Assumes the proportional odds assumption.\n",
    "\n",
    "2. **Random Forest Classifier for Ordinal Regression:**\n",
    "   - **Type:** Supervised learning, classification.\n",
    "   - **Strengths:** Handles non-linearity and interactions, robust to overfitting, and can handle ordinal targets by using the `OrdinalClassifier` from scikit-learn.\n",
    "   - **Considerations:** Might not be as interpretable as ordinal logistic regression.\n",
    "\n",
    "3. **Support Vector Machines (SVM) for Ordinal Classification:**\n",
    "   - **Type:** Supervised learning, ordinal classification.\n",
    "   - **Strengths:** Effective in high-dimensional spaces, versatile, and can be adapted for ordinal targets.\n",
    "   - **Considerations:** Sensitive to the choice of kernel function and parameters.\n",
    "\n",
    "4. **Ordinal Neural Networks:**\n",
    "   - **Type:** Deep learning, ordinal classification.\n",
    "   - **Strengths:** Can capture complex relationships, automatically learns feature representations, and can be adapted for ordinal outcomes.\n",
    "   - **Considerations:** Requires more data, computationally intensive, and might be overkill for simpler problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7bb71",
   "metadata": {},
   "source": [
    "# Preparing Test Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = pd.read_csv('daasbstp2023/energia_202301-202304.csv', na_filter=False, encoding = \"latin\")\n",
    "test_me = pd.read_csv('daasbstp2023/meteo_202301-202304.csv', na_filter=False, encoding = \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32313b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1feb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_me.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_me.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864875ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for each column in the Energy dataset.\n",
    "for column in test_en.columns:\n",
    "    unique_values = test_en[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for each column in the Meteorology dataset\n",
    "\n",
    "for column in test_me.columns:\n",
    "    unique_values = test_me[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_me = test_me.drop(['city_name', 'sea_level', 'grnd_level'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d523cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to unified format\n",
    "test_en['datetime'] = pd.to_datetime(test_en['Data'] + ' ' + test_en['Hora'].astype(str) + ':00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Drop the original 'Data' and 'Hora' columns if needed\n",
    "test_en = test_en.drop(['Data', 'Hora'], axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "test_en.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db057d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datetime in test_me (loses the UTC tag)\n",
    "test_me['dt_iso'] = pd.to_datetime(test_me['dt_iso'], format='%Y-%m-%d %H:%M:%S %z UTC')\n",
    "test_me['dt_iso'] = test_me['dt_iso'].dt.tz_localize(None)\n",
    "test_me = test_me.rename(columns={\"dt_iso\": \"datetime\"})\n",
    "test_me.iloc[801].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1549b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop the 'dt' column as it is redundant\n",
    "test_me = test_me.drop(['dt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0521056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the dataframes by datetime so we can detect any time skips\n",
    "test_en = test_en.sort_values(by=['datetime'])\n",
    "test_me = test_me.sort_values(by=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_en = df_en['datetime'].diff()\n",
    "time_diff_me = df_me['datetime'].diff()\n",
    "\n",
    "# Print the irregular time intervals\n",
    "irregularities_en = time_diff_en[time_diff_en != '0 days 01:00:00']\n",
    "irregularities_me = time_diff_me[time_diff_me != '0 days 01:00:00']\n",
    "print(\"Irregular time intervals in df_en:\")\n",
    "print(irregularities_en)\n",
    "print(\"\\n\")\n",
    "print(\"Irregular time intervals in df_me:\")\n",
    "print(irregularities_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d034751",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_test_df = pd.merge(test_en, test_me, on='datetime', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa976e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = outer_join_test_df\n",
    "\n",
    "test_merged['rain_1h'] = pd.to_numeric(test_merged['rain_1h'].replace('', '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb626c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Encoding for 'weather_description'\n",
    "test_merged = pd.get_dummies(test_merged, columns=['weather_description'], prefix='weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54705e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_df['datetime'] and test_merged['datetime'] are in datetime format\n",
    "test_merged_filled = test_merged.copy()\n",
    "\n",
    "# Iterate over rows in test_merged\n",
    "for index, row in test_merged.iterrows():\n",
    "    if pd.isna(row['temp']):\n",
    "        # Find the corresponding row in merged_df for the same hour on previous days\n",
    "        matching_rows = merged_df[\n",
    "            (merged_df['datetime'].dt.hour == row['datetime'].hour) &\n",
    "            (merged_df['datetime'] < row['datetime'])\n",
    "        ]\n",
    "\n",
    "        # Check if matching_rows is not empty\n",
    "        if not matching_rows.empty:\n",
    "            # Use the values from the last available row\n",
    "            matching_row = matching_rows.iloc[-1]\n",
    "\n",
    "            # Update the missing values in test_merged with values from merged_df\n",
    "            test_merged_filled.loc[index, ['temp', 'feels_like', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'clouds_all']] = matching_row[['temp', 'feels_like', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'clouds_all']]\n",
    "\n",
    "# Now, test_merged_filled should have missing values filled with corresponding values from merged_df\n",
    "test_merged_filled.info()\n",
    "\n",
    "test_merged_filled.iloc[2200].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each unique value in merged_df['Injection']\n",
    "injection_counts = merged_df['Injection'].value_counts()\n",
    "print(injection_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec60d2",
   "metadata": {},
   "source": [
    "# **MODEL TRAINING**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime features in training dataset\n",
    "merged_df['year'] = merged_df['datetime'].dt.year\n",
    "merged_df['month'] = merged_df['datetime'].dt.month\n",
    "merged_df['day'] = merged_df['datetime'].dt.day\n",
    "merged_df['hour'] = merged_df['datetime'].dt.hour\n",
    "\n",
    "# Drop the original datetime column\n",
    "merged_df = merged_df.drop('datetime', axis=1)\n",
    "\n",
    "# Convert datetime features in test dataset\n",
    "test_merged_filled['year'] = test_merged_filled['datetime'].dt.year\n",
    "test_merged_filled['month'] = test_merged_filled['datetime'].dt.month\n",
    "test_merged_filled['day'] = test_merged_filled['datetime'].dt.day\n",
    "test_merged_filled['hour'] = test_merged_filled['datetime'].dt.hour\n",
    "\n",
    "# Drop the original datetime column\n",
    "test_merged_filled = test_merged_filled.drop('datetime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43e55a",
   "metadata": {},
   "source": [
    "# First Model: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'Injection' is the target variable\n",
    "X = merged_df.drop(['Injection'], axis=1)\n",
    "y = merged_df['Injection']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=232)\n",
    "\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=44253)\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # You can adjust the range of values\n",
    "    'max_depth': [None, 10, 20],      # You can adjust the range of values\n",
    "    'min_samples_split': [2, 5, 10],  # You can adjust the range of values\n",
    "    'min_samples_leaf': [1, 2, 4]      # You can adjust the range of values\n",
    "}\n",
    "\n",
    "# Perform GridSearch with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearch\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "val_predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the best model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f42412",
   "metadata": {},
   "source": [
    "## Alternative Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b19c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'Injection' is the target variable\n",
    "X = merged_df.drop(['Injection'], axis=1)\n",
    "y = merged_df['Injection']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=232)\n",
    "\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=4253, max_depth=None, min_samples_leaf=1)\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [250, 300, 200],  # You can adjust the range of values\n",
    "    'min_samples_split': [4, 5, 6],  # You can adjust the range of values\n",
    "}\n",
    "\n",
    "# Perform GridSearch with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearch\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "val_predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the best model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d99706",
   "metadata": {},
   "source": [
    "### Using the hyperparameters obtained from the RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3019ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Best hyperparameters from GridSearchCV\n",
    "best_params = {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250}\n",
    "\n",
    "# Create a Random Forest model with the best hyperparameters\n",
    "best_model = RandomForestClassifier(\n",
    "    random_state=4253,\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf']\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccf249",
   "metadata": {},
   "source": [
    "## gridsearch but with class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'Injection' is the target variable\n",
    "X = merged_df.drop(['Injection'], axis=1)\n",
    "y = merged_df['Injection']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=232)\n",
    "\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=4353, class_weight='balanced')\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # You can adjust the range of values\n",
    "    'max_depth': [None, 10, 20],      # You can adjust the range of values\n",
    "    'min_samples_split': [2, 5, 10],  # You can adjust the range of values\n",
    "    'min_samples_leaf': [1, 2, 4]      # You can adjust the range of values\n",
    "}\n",
    "\n",
    "# Perform GridSearch with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearch\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "val_predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the best model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50354b",
   "metadata": {},
   "source": [
    "## Using the parameters from the seach with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Best hyperparameters from GridSearchCV\n",
    "best_params = {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "# Create a Random Forest model with the best hyperparameters\n",
    "best_model = RandomForestClassifier(\n",
    "    random_state=4353,\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model: {val_accuracy}')\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Get the names of the features\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to organize the feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc936dd2",
   "metadata": {},
   "source": [
    "# Droping features according to their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RandomForestClassifier with best hyperparameters\n",
    "best_model = RandomForestClassifier(\n",
    "    random_state=4353,\n",
    "    n_estimators=150,\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Get the names of the features\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to organize the feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Print features with importance less than a threshold (e.g., 0.01)\n",
    "threshold = 0.01\n",
    "removed_features = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature']\n",
    "print(f\"Features with Importance < {threshold} being removed: {removed_features.tolist()}\")\n",
    "\n",
    "# Filter out features with importance less than the threshold\n",
    "selected_features = feature_importance_df[feature_importance_df['Importance'] >= threshold]['Feature']\n",
    "\n",
    "# Subset the training and validation data with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_val_selected = X_val[selected_features]\n",
    "\n",
    "# Fit the model on the data with selected features\n",
    "best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = best_model.predict(X_val_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy with Best Model and Selected Features: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['weather_broken clouds', 'weather_few clouds', 'weather_heavy intensity rain',\n",
    "                    'weather_light rain', 'weather_moderate rain', 'weather_overcast clouds',\n",
    "                    'weather_scattered clouds', 'weather_sky is clear', 'year']\n",
    "\n",
    "\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RandomForestClassifier with best hyperparameters\n",
    "base_model = RandomForestClassifier(\n",
    "    random_state=4353,\n",
    "    n_estimators=150,\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Bagging: Create a BaggingClassifier with the RandomForest as the base estimator\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=10, random_state=2023)\n",
    "\n",
    "# Fit the bagging model on the entire training data\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions_bagging = bagging_model.predict(X_val)\n",
    "\n",
    "# Evaluate the bagging model\n",
    "val_accuracy_bagging = accuracy_score(y_val, val_predictions_bagging)\n",
    "print(f'Validation Accuracy with Bagging and Best Model: {val_accuracy_bagging}')\n",
    "\n",
    "# Boosting: Create an AdaBoostClassifier with the RandomForest as the base estimator\n",
    "boosting_model = AdaBoostClassifier(base_model, n_estimators=50, random_state=2023)\n",
    "\n",
    "# Fit the boosting model on the entire training data\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions_boosting = boosting_model.predict(X_val)\n",
    "\n",
    "# Evaluate the boosting model\n",
    "val_accuracy_boosting = accuracy_score(y_val, val_predictions_boosting)\n",
    "print(f'Validation Accuracy with Boosting and Best Model: {val_accuracy_boosting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37838de4",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use LabelEncoder to convert string labels to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Create an XGBoost classifier with best hyperparameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=42069,\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=1,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1\n",
    ")\n",
    "\n",
    "# Fit the XGBoost model on the entire training data\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Inverse transform the predictions to get original labels\n",
    "val_predictions_original = label_encoder.inverse_transform(val_predictions_xgb)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "val_accuracy_xgb = accuracy_score(y_val, val_predictions_original)\n",
    "print(f'Validation Accuracy with XGBoost and Best Model: {val_accuracy_xgb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb619ac",
   "metadata": {},
   "source": [
    "# Model Prediction on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1967e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_merged_filled = test_merged.copy()\n",
    "\n",
    "# # Convert datetime features in test dataset\n",
    "# test_merged_filled['year'] = test_merged_filled['datetime'].dt.year\n",
    "# test_merged_filled['month'] = test_merged_filled['datetime'].dt.month\n",
    "# test_merged_filled['day'] = test_merged_filled['datetime'].dt.day\n",
    "# test_merged_filled['hour'] = test_merged_filled['datetime'].dt.hour\n",
    "\n",
    "# # Drop the original datetime column\n",
    "# test_merged_filled = test_merged_filled.drop('datetime', axis=1)\n",
    "\n",
    "test_merged_filled = test_merged_filled.drop(columns=columns_to_drop)\n",
    "\n",
    "# test_merged_filled.fillna(test_merged_filled.mean(), inplace=True)\n",
    "\n",
    "# test_predictions = best_model.predict(test_merged_filled)\n",
    "\n",
    "# submission_df = pd.DataFrame({'RowId': test_merged_filled.index + 1, 'Result': test_predictions})\n",
    "\n",
    "# submission_df.to_csv('predictions/submission_rf_weighted.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = xgb_model.predict(test_merged_filled)\n",
    "\n",
    "# Convert numeric predictions to class labels\n",
    "class_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Create a DataFrame for the submission file\n",
    "submission_df = pd.DataFrame({'RowId': test_merged_filled.index + 1, 'Result': class_labels})\n",
    "\n",
    "# Save the submission file to the 'predictions' folder with column labels\n",
    "submission_df.to_csv('predictions/submission_xgboost3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
