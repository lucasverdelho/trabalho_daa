{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASETS:  https://www.kaggle.com/competitions/daasbstp2023/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by importing all packages needed.\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import time\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.stats import uniform, poisson\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, \\\n",
    "                            accuracy_score, \\\n",
    "                            precision_score, \\\n",
    "                            recall_score, \\\n",
    "                            f1_score, \\\n",
    "                            fbeta_score, \\\n",
    "                            mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            roc_auc_score, \\\n",
    "                            roc_curve, \\\n",
    "                            classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4000a",
   "metadata": {},
   "source": [
    "# **Sample and Assess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets.\n",
    "df_en1 = pd.read_csv('daasbstp2023/energia_202109-202112.csv', na_filter=False, encoding = \"latin\")\n",
    "df_en2 = pd.read_csv('daasbstp2023/energia_202201-202212.csv', na_filter=False, encoding = \"latin\")\n",
    "df_me1 = pd.read_csv('daasbstp2023/meteo_202109-202112.csv', na_filter=False, encoding = \"latin\")\n",
    "df_me2 = pd.read_csv('daasbstp2023/meteo_202201-202212.csv', na_filter=False, encoding = \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ff5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the datasets by type.\n",
    "df_en = df_en1._append(df_en2,ignore_index=True)\n",
    "df_me = df_me1._append(df_me2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfea1f",
   "metadata": {},
   "source": [
    "## Energy Consumption Dataset\n",
    "\n",
    "This dataset provides information on energy consumption recorded at different timestamps. Each record includes the following columns:\n",
    "\n",
    "- **Data (Date):** The timestamp associated with the record, indicating the date.\n",
    "- **Hora (Hour):** The hour associated with the record.\n",
    "- **Normal (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), from the electrical grid during a normal period in daily bi-hourly cycles (non-off-peak hours).\n",
    "- **Horário Económico (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), from the electrical grid during an economic period in daily bi-hourly cycles (off-peak hours).\n",
    "- **Autoconsumo (kWh):** The amount of electrical energy consumed, in kilowatt-hours (kWh), generated from solar panels (self-consumption).\n",
    "- **Injeção na rede (kWh):** A qualitative feature indicating the level of energy injection into the grid on a scale of None, Low, Medium, High, and Very High.\n",
    "\n",
    "This dataset is utilized in a competition and includes various features, with particular emphasis on the \"Injeção na rede (kWh)\" feature, which provides information about the qualitative scale of energy injection into the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb209be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79924f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80133a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for each column in the Energy dataset.\n",
    "for column in df_en.columns:\n",
    "    unique_values = df_en[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105695bf",
   "metadata": {},
   "source": [
    "## Weather Information Dataset\n",
    "\n",
    "This dataset provides comprehensive information related to weather conditions at different timestamps. The columns in the dataset include:\n",
    "\n",
    "- **dt (Timestamp):** The timestamp associated with the record.\n",
    "- **dt_iso (ISO Date):** The date associated with the record, accurate to the second.\n",
    "- **city_name:** The location for which weather data is recorded.\n",
    "- **temp (Temperature):** The temperature in degrees Celsius.\n",
    "- **feels_like (Feels Like):** The perceived temperature in degrees Celsius.\n",
    "- **temp_min (Min Temperature):** The minimum felt temperature in degrees Celsius.\n",
    "- **temp_max (Max Temperature):** The maximum felt temperature in degrees Celsius.\n",
    "- **pressure:** Atmospheric pressure in atmospheres.\n",
    "- **sea_level:** Atmospheric pressure at sea level in atmospheres.\n",
    "- **grnd_level:** Atmospheric pressure at local altitude in atmospheres.\n",
    "- **humidity:** Humidity level as a percentage.\n",
    "- **wind_speed:** Wind speed in meters per second.\n",
    "- **rain_1h (Rainfall):** Average precipitation value in the last hour.\n",
    "- **clouds_all (Cloudiness):** Cloud coverage percentage.\n",
    "- **weather_description:** Qualitative assessment of weather conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c29965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_me.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a634c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_me.iloc[801].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_me.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for each column in the Energy dataset.\n",
    "for column in df_me.columns:\n",
    "    unique_values = df_me[column].unique()\n",
    "    print(f\"{column}, Number of Unique Values: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ae71f",
   "metadata": {},
   "source": [
    "# **Data Processing**\n",
    "\n",
    "After Loading the data, we must now process it in order to be able to use it in our model. For this we will follow the following steps:\n",
    "1. Remove unnecessary columns\n",
    "2. Handle the Date column on both datasets and unify the format\n",
    "3. Join the datasets\n",
    "4. Handle the missing values or rows (dates that are not present in both datasets) if any\n",
    "5. Handle the categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop city_name, sea_level and grnd_level as they only have one unique value\n",
    "df_me = df_me.drop(['city_name', 'sea_level', 'grnd_level'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55227cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert columns to categorical\n",
    "# df_en[\"Injeção na rede (kWh)\"] = df_en[\"Injeção na rede (kWh)\"].astype(\"category\")\n",
    "# df_me[\"weather_description\"] = df_me['weather_description'].astype(\"category\")\n",
    "\n",
    "# # For \"Injeção na rede (kWh)\" column in df_en\n",
    "# print(\"Categories for 'Injeção na rede (kWh)' in df_en:\")\n",
    "# print(df_en[\"Injeção na rede (kWh)\"].cat.categories)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # For \"weather_description\" column in df_me\n",
    "# print(\"Categories for 'weather_description' in df_me:\")\n",
    "# print(df_me[\"weather_description\"].cat.categories)\n",
    "# print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to unified format\n",
    "df_en['datetime'] = pd.to_datetime(df_en['Data'] + ' ' + df_en['Hora'].astype(str) + ':00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Drop the original 'Data' and 'Hora' columns if needed\n",
    "df_en = df_en.drop(['Data', 'Hora'], axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a60faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix datetime in df_me (loses the UTC tag)\n",
    "df_me['dt_iso'] = pd.to_datetime(df_me['dt_iso'], format='%Y-%m-%d %H:%M:%S %z UTC')\n",
    "df_me['dt_iso'] = df_me['dt_iso'].dt.tz_localize(None)\n",
    "df_me = df_me.rename(columns={\"dt_iso\": \"datetime\"})\n",
    "df_me.iloc[801].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop the 'dt' column as it is redundant\n",
    "df_me = df_me.drop(['dt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the dataframes by datetime so we can detect any time skips\n",
    "df_en = df_en.sort_values(by=['datetime'])\n",
    "df_me = df_me.sort_values(by=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b76b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_en = df_en['datetime'].diff()\n",
    "time_diff_me = df_me['datetime'].diff()\n",
    "\n",
    "# Print the irregular time intervals\n",
    "irregularities_en = time_diff_en[time_diff_en != '0 days 01:00:00']\n",
    "irregularities_me = time_diff_me[time_diff_me != '0 days 01:00:00']\n",
    "print(\"Irregular time intervals in df_en:\")\n",
    "print(irregularities_en)\n",
    "print(\"\\n\")\n",
    "print(\"Irregular time intervals in df_me:\")\n",
    "print(irregularities_me)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be20a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_merged_df = pd.merge(df_en, df_me, on='datetime', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602b4f6",
   "metadata": {},
   "source": [
    "Since the datetime data exhibited no irregularities, the additional entries present in the Weather dataset but not in the Energy dataset can be attributed to the Weather dataset containing data from days before or after the Energy dataset's first or last entry, respectively. A manual analysis of the dataset reveals that the Weather dataset includes entries starting from 2021-09-01, while the Energy dataset commences from 2021-09-29. Consequently, it is necessary to exclude entries from the Weather dataset that precede 2021-09-29, as they will not contribute to the modeling process. In order to achieve this we will do an Inner Join between the two datasets on the datetime column, and the resulting dataset will be the one we use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point, if I do an inner join I am losing 5.7% of all datarows\n",
    "merged_df = pd.merge(df_en, df_me, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sava the clean dataset\n",
    "merged_df.to_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'Injeção na rede (kWh)': 'Injection'}, inplace=True)\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805aa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec017bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'rain_1h' column to float and replace empty strings with 0\n",
    "merged_df['rain_1h'] = pd.to_numeric(merged_df['rain_1h'].replace('', '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ce984",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_description_counts = merged_df['weather_description'].value_counts()\n",
    "print(weather_description_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Encoding for 'weather_description'\n",
    "merged_df = pd.get_dummies(merged_df, columns=['weather_description'], prefix='weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'weather_description' column\n",
    "merged_df = merged_df.drop('weather_description', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e041f6",
   "metadata": {},
   "source": [
    "# ALTERNATIVE PATH\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa57025",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_testing = merged_df.copy()\n",
    "order_mapping = {'None': 0, 'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "\n",
    "# Map the categorical values to numerical values using the specified order\n",
    "merged_df_testing['Injection'] = merged_df_testing['Injection'].map(order_mapping)\n",
    "\n",
    "\n",
    "merged_df_testing.iloc[901].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Specify the order for 'Injeção na Rede'\n",
    "injecao_order = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Box plot with 'clouds_all' on the x-axis and 'Injeção na Rede' on the y-axis\n",
    "sns.boxplot(x='clouds_all', y='Injection', data=merged_df_testing, order=injecao_order, palette='viridis')\n",
    "\n",
    "plt.title('Injeção na Rede vs. Cloud Coverage')\n",
    "plt.xlabel('Cloud Coverage (%)')\n",
    "plt.ylabel('Injeção na Rede')\n",
    "\n",
    "# Manually create a legend to the right side\n",
    "legend_labels = ['None', 'Low', 'Medium', 'High', 'Very High']\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=f'{label} ({value})', markerfacecolor=color, markersize=10) for value, (label, color) in enumerate(zip(legend_labels, sns.color_palette('viridis', n_colors=len(legend_labels))))]\n",
    "\n",
    "plt.legend(handles=legend_handles, title='Injeção na Rede', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Explicitly show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'rain_1h' column to float and replace empty strings with 0\n",
    "merged_df['rain_1h'] = pd.to_numeric(merged_df['rain_1h'].replace('', '0'))\n",
    "\n",
    "# Exclude 'Injection' and 'weather_description' columns from the correlation matrix\n",
    "columns_to_exclude = ['Injection', 'weather_description']\n",
    "correlation_matrix = merged_df.drop(columns_to_exclude, axis=1).corr()\n",
    "\n",
    "# Generate a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix (Excluding \"Injection\" and \"weather_description\")')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f708a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Injection' is a categorical column with order: None, Low, Medium, High, Very High\n",
    "injection_order = ['None', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "# Set the order of the 'Injection' column\n",
    "merged_df['Injection'] = pd.Categorical(merged_df['Injection'], categories=injection_order, ordered=True)\n",
    "\n",
    "# Create subplots with 4 plots per row\n",
    "num_columns = 4\n",
    "num_rows = (len(merged_df.columns) - 1) // num_columns + 1\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(16, 4 * num_rows))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over numerical columns (excluding 'Injection')\n",
    "numerical_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x='Injection', y=column, data=merged_df, ax=ax, order=injection_order)\n",
    "    ax.set_title(f'Box Plot of {column} by Injection')\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_columns), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
